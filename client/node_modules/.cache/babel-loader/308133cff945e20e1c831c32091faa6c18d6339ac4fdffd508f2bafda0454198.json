{"ast":null,"code":"var _s = $RefreshSig$();\nimport axios from 'axios';\nimport cheerio from 'cheerio';\nimport fs from 'fs';\nimport { useDispatch } from 'react-redux';\nimport { useSelector } from 'react-redux';\nexport default function crawlAction(initUrl = \"https://www.wikipedia.org\") {\n  _s();\n  const isCrawling = useSelector(state => state.crawl.isCrawling);\n  const dispatch = useDispatch();\n  if (!isCrawling) {\n    return;\n  }\n  const dbName = \"title-link-score\";\n  const collectionName = \"wikipedia\";\n  function hashCode(s) {\n    for (var i = 0, h = 0; i < s.length; i++) h = Math.imul(31, h) + s.charCodeAt(i) | 0;\n    return h;\n  }\n\n  // What we need to do for scraping\n  // 1. Visit the website\n  // 2. Get the HTML from the website (cheerio)\n  // 3. Find the element we want to scrape -> stort in DB ( we will use fs )\n  // 4. Find the next page url -> rank pages by reference\n  // 5. Go to the next page and repeat\n\n  let disallowedList = {\n    [\"/wiki/MediaWiki:Spam-blacklist\"]: {\n      'linkSegment': '/wiki/MediaWiki:Spam-blacklist'\n    }\n  };\n  let queue = [];\n  let progressIndex = 0;\n\n  /**\n   * crawl function\n   * @param {*} url \n   * @returns \n   */\n  const crawl = async url => {\n    console.log(`Visiting ${url}`);\n    console.log('Number of URL in queue: ', queue.length - progressIndex);\n    try {\n      var htmlDoc = await axios.get(url);\n    } catch (e) {\n      await startNextQueue();\n      return;\n    }\n    if (!htmlDoc.data) {\n      await startNextQueue();\n      return;\n    }\n    const $ = cheerio.load(htmlDoc.data); // conventional $ notation\n    const links = $('a'); // get all links from the page\n    const title = $(\"h1\").text(); // get title of the page\n\n    // check if the url is allowed to be crawled\n    const isAllowed = await checkRobotTXT(url);\n    if (!isAllowed) {\n      await startNextQueue();\n      return;\n    }\n    links.each((index, link) => {\n      const href = $(link).attr('href');\n      if (!href) return;\n\n      // if href starts with http or https\n      if (href.startsWith('http://') || href.startsWith('https://')) {\n        //create new data\n        newData = {\n          id: hashCode(url),\n          title: title,\n          link: url,\n          score: 1\n        };\n        checkVisitedBeforePush(newData);\n        return;\n      }\n\n      // if not, it is a relative link\n      const originUrl = new URL(url).origin;\n      const newUrl = originUrl + href;\n      //create new data\n      newData = {\n        id: hashCode(newUrl),\n        title: title,\n        link: newUrl,\n        score: 1\n      };\n      checkVisitedBeforePush(newData);\n    });\n    if (queue[progressIndex]) {\n      await startNextQueue();\n    } else {\n      console.log('Finished crawling');\n    }\n  };\n\n  /**\n   * Check if the url is visited before check in mongodb\n   * @param {object} data\n   * @returns {boolean}\n   */\n  async function checkVisitedBeforePush(data) {\n    await fetch(`http://localhost:3000/record/${data.id}`, {\n      method: \"GET\"\n    }, function (err, res) {\n      if (err) throw err;\n      if (res) {\n        if (res == null) {\n          queue.push(data);\n        } else {\n          // update score\n          res.score += 1;\n          updateScore(res);\n        }\n      }\n    });\n  }\n\n  /**\n   * Start crawling the next url in the queue\n   */\n  const startNextQueue = async () => {\n    await timeout();\n    if (isCrawling) {\n      crawl(queue[progressIndex]);\n      storeDb(queue[progressIndex]);\n      progressIndex += 1;\n    } else {\n      return;\n    }\n  };\n\n  /**\n   * Timeout function\n   */\n  const timeout = () => {\n    return new Promise((resolve, reject) => {\n      setTimeout(() => {\n        resolve();\n      }, 300);\n    });\n  };\n\n  /**\n   * Store new data in mongodb\n   */\n  async function storeDb(data) {\n    await fetch(`http://localhost:3000/record/add`, {\n      method: \"POST\",\n      body: JSON.stringify(data)\n    }, function (err, res) {\n      if (err) throw err;\n      response.json(res);\n    });\n  }\n\n  /**\n   * Update score in mongodb\n   * @param {object} data\n   * @returns {boolean}\n   */\n  async function updateScore(data) {\n    await fetch(`http://localhost:3000/update/${data.id}`, {\n      method: \"POST\",\n      body: JSON.stringify(data)\n    }, function (err, res) {\n      if (err) throw err;\n      response.json(res);\n    });\n  }\n\n  /**\n   * Check if the url is allowed to be crawled based on disallowedList\n   * @param {string} currentUrl\n   * @returns {boolean}\n   */\n  const checkRobotTXT = async currentUrl => {\n    let isAllowed = true;\n    for (const key in disallowedList) {\n      if (currentUrl.includes(key)) {\n        isAllowed = false;\n        break;\n      }\n    }\n    return isAllowed;\n  };\n\n  /**\n   * Store disallowedList in a file\n   * @param {string} mainUrl\n   */\n  const storeRobotsTxt = async mainUrl => {\n    const robotUrl = mainUrl + '/robots.txt';\n    const robotTxt = await fetch(robotUrl);\n    const robotTxtText = await robotTxt.text();\n    const robotTxtLines = robotTxtText.split('\\n');\n    robotTxtLines.forEach(line => {\n      if (line.includes('Disallow:')) {\n        const disallowedUrl = line.split(' ')[1];\n        if (!disallowedUrl) {\n          console.log('Error Disallowed url text');\n        } else {\n          if (!disallowedUrl.includes('/wiki/')) {\n            //continue to next iteration\n            return;\n          }\n          console.log('Disallowed url text', disallowedUrl);\n          // store disallowedList in a file\n          disallowedList[disallowedUrl] = {\n            linkSegment: disallowedUrl\n          };\n          const json = JSON.stringify(disallowedList);\n          fs.writeFileSync('./disallowed.json', json);\n        }\n      }\n    });\n  };\n  storeRobotsTxt(initUrl);\n  queue.push(initUrl);\n}\n_s(crawlAction, \"UMFhAXZcMHuEO2P/OYVCp3ZKXnw=\", false, function () {\n  return [useSelector, useDispatch];\n});","map":{"version":3,"names":["axios","cheerio","fs","useDispatch","useSelector","crawlAction","initUrl","_s","isCrawling","state","crawl","dispatch","dbName","collectionName","hashCode","s","i","h","length","Math","imul","charCodeAt","disallowedList","queue","progressIndex","url","console","log","htmlDoc","get","e","startNextQueue","data","$","load","links","title","text","isAllowed","checkRobotTXT","each","index","link","href","attr","startsWith","newData","id","score","checkVisitedBeforePush","originUrl","URL","origin","newUrl","fetch","method","err","res","push","updateScore","timeout","storeDb","Promise","resolve","reject","setTimeout","body","JSON","stringify","response","json","currentUrl","key","includes","storeRobotsTxt","mainUrl","robotUrl","robotTxt","robotTxtText","robotTxtLines","split","forEach","line","disallowedUrl","linkSegment","writeFileSync"],"sources":["/Users/kimdo/Documents/Projects/mini-browser/client/src/components/crawl/crawlAction.js"],"sourcesContent":["import axios from 'axios';\nimport cheerio from 'cheerio';\nimport fs from 'fs';\nimport { useDispatch } from 'react-redux';\nimport { useSelector } from 'react-redux';\nexport default function crawlAction(initUrl = \"https://www.wikipedia.org\") {\n    const isCrawling = useSelector((state) => state.crawl.isCrawling);\n    const dispatch = useDispatch();\n    \n    if (!isCrawling) {\n        return;\n    }\n    const dbName = \"title-link-score\";\n    const collectionName = \"wikipedia\";\n    \n    function hashCode(s) {\n        for (var i = 0, h = 0; i < s.length; i++)\n            h = Math.imul(31, h) + s.charCodeAt(i) | 0;\n        return h;\n    }\n    \n    // What we need to do for scraping\n    // 1. Visit the website\n    // 2. Get the HTML from the website (cheerio)\n    // 3. Find the element we want to scrape -> stort in DB ( we will use fs )\n    // 4. Find the next page url -> rank pages by reference\n    // 5. Go to the next page and repeat\n    \n    let disallowedList = {\n        [\"/wiki/MediaWiki:Spam-blacklist\"]: {\n            'linkSegment': '/wiki/MediaWiki:Spam-blacklist',\n        }\n    };\n    \n    let queue = [];\n    let progressIndex = 0;\n    \n    /**\n     * crawl function\n     * @param {*} url \n     * @returns \n     */\n    const crawl = async (url) => {\n        console.log(`Visiting ${url}`);\n        console.log('Number of URL in queue: ', queue.length - progressIndex);\n        try {\n            var htmlDoc = await axios.get(url);\n        } catch (e) {\n            await startNextQueue();\n            return;\n        }\n    \n    \n        if (!htmlDoc.data) {\n            await startNextQueue();\n            return;\n        }\n    \n        const $ = cheerio.load(htmlDoc.data); // conventional $ notation\n        const links = $('a'); // get all links from the page\n        const title = $(\"h1\").text(); // get title of the page\n    \n        // check if the url is allowed to be crawled\n        const isAllowed = await checkRobotTXT(url);\n        if (!isAllowed) {\n            await startNextQueue();\n            return;\n        }\n    \n        links.each((index, link) => {\n            const href = $(link).attr('href');\n    \n            if (!href) return;\n    \n            // if href starts with http or https\n            if (href.startsWith('http://') || href.startsWith('https://')) {\n                //create new data\n                newData = {\n                    id: hashCode(url),\n                    title: title,\n                    link: url,\n                    score: 1,\n                }\n                checkVisitedBeforePush(newData);\n                return;\n            }\n    \n            // if not, it is a relative link\n            const originUrl = new URL(url).origin;\n            const newUrl = originUrl + href;\n            //create new data\n            newData = {\n                id: hashCode(newUrl),\n                title: title,\n                link: newUrl,\n                score: 1,\n            }\n            checkVisitedBeforePush(newData);\n        });\n    \n        if (queue[progressIndex]) {\n            await startNextQueue();\n        } else {\n            console.log('Finished crawling');\n        }\n    };\n    \n    /**\n     * Check if the url is visited before check in mongodb\n     * @param {object} data\n     * @returns {boolean}\n     */\n    async function checkVisitedBeforePush(data) {\n        await fetch(`http://localhost:3000/record/${data.id}`, {\n            method: \"GET\",\n        }, function (err, res) {\n            if (err) throw err;\n            if (res) {\n                if (res == null) {\n                    queue.push(data);\n                } else {\n                    // update score\n                    res.score += 1;\n                    updateScore(res);\n                }\n            }\n        }\n        );\n    \n    \n    }\n    \n    \n    \n    /**\n     * Start crawling the next url in the queue\n     */\n    const startNextQueue = async () => {\n        await timeout();\n        if (isCrawling) {\n        crawl(queue[progressIndex]);\n        storeDb(queue[progressIndex]);\n        progressIndex += 1;\n        } else {\n            return;\n        }\n    }\n    \n    /**\n     * Timeout function\n     */\n    const timeout = () => {\n        return new Promise((resolve, reject) => {\n            setTimeout(() => {\n                resolve();\n            }, 300);\n        })\n    }\n    \n    /**\n     * Store new data in mongodb\n     */\n    async function storeDb(data) {\n        await fetch(`http://localhost:3000/record/add`, {\n            method: \"POST\",\n            body: JSON.stringify(data),\n        }, function (err, res) {\n            if (err) throw err;\n            response.json(res);\n        }\n        );\n    }\n    \n    /**\n     * Update score in mongodb\n     * @param {object} data\n     * @returns {boolean}\n     */\n    async function updateScore(data) {\n        await fetch(`http://localhost:3000/update/${data.id}`, {\n            method: \"POST\",\n            body: JSON.stringify(data),\n        }, function (err, res) {\n            if (err) throw err;\n            response.json(res);\n        }\n        );\n    }\n    \n    /**\n     * Check if the url is allowed to be crawled based on disallowedList\n     * @param {string} currentUrl\n     * @returns {boolean}\n     */\n    const checkRobotTXT = async (currentUrl) => {\n        let isAllowed = true;\n        for (const key in disallowedList) {\n            if (currentUrl.includes(key)) {\n                isAllowed = false;\n                break;\n            }\n        }\n        return isAllowed;\n    }\n    \n    /**\n     * Store disallowedList in a file\n     * @param {string} mainUrl\n     */\n    const storeRobotsTxt = async (mainUrl) => {\n        const robotUrl = mainUrl + '/robots.txt';\n        const robotTxt = await fetch(robotUrl);\n        const robotTxtText = await robotTxt.text();\n        const robotTxtLines = robotTxtText.split('\\n');\n        robotTxtLines.forEach(line => {\n            if (line.includes('Disallow:')) {\n                const disallowedUrl = line.split(' ')[1];\n    \n    \n                if (!disallowedUrl) {\n                    console.log('Error Disallowed url text');\n                } else {\n                    if (!disallowedUrl.includes('/wiki/')) {\n                        //continue to next iteration\n                        return;\n                    }\n                    console.log('Disallowed url text', disallowedUrl);\n                    // store disallowedList in a file\n                    disallowedList[disallowedUrl] = {\n                        linkSegment: disallowedUrl,\n                    }\n                    const json = JSON.stringify(disallowedList);\n                    fs.writeFileSync('./disallowed.json', json);\n                }\n            }\n        });\n    }\n    storeRobotsTxt(initUrl);\n    queue.push(initUrl);\n}\n\n\n"],"mappings":";AAAA,OAAOA,KAAK,MAAM,OAAO;AACzB,OAAOC,OAAO,MAAM,SAAS;AAC7B,OAAOC,EAAE,MAAM,IAAI;AACnB,SAASC,WAAW,QAAQ,aAAa;AACzC,SAASC,WAAW,QAAQ,aAAa;AACzC,eAAe,SAASC,WAAWA,CAACC,OAAO,GAAG,2BAA2B,EAAE;EAAAC,EAAA;EACvE,MAAMC,UAAU,GAAGJ,WAAW,CAAEK,KAAK,IAAKA,KAAK,CAACC,KAAK,CAACF,UAAU,CAAC;EACjE,MAAMG,QAAQ,GAAGR,WAAW,CAAC,CAAC;EAE9B,IAAI,CAACK,UAAU,EAAE;IACb;EACJ;EACA,MAAMI,MAAM,GAAG,kBAAkB;EACjC,MAAMC,cAAc,GAAG,WAAW;EAElC,SAASC,QAAQA,CAACC,CAAC,EAAE;IACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEC,CAAC,GAAG,CAAC,EAAED,CAAC,GAAGD,CAAC,CAACG,MAAM,EAAEF,CAAC,EAAE,EACpCC,CAAC,GAAGE,IAAI,CAACC,IAAI,CAAC,EAAE,EAAEH,CAAC,CAAC,GAAGF,CAAC,CAACM,UAAU,CAACL,CAAC,CAAC,GAAG,CAAC;IAC9C,OAAOC,CAAC;EACZ;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA,IAAIK,cAAc,GAAG;IACjB,CAAC,gCAAgC,GAAG;MAChC,aAAa,EAAE;IACnB;EACJ,CAAC;EAED,IAAIC,KAAK,GAAG,EAAE;EACd,IAAIC,aAAa,GAAG,CAAC;;EAErB;AACJ;AACA;AACA;AACA;EACI,MAAMd,KAAK,GAAG,MAAOe,GAAG,IAAK;IACzBC,OAAO,CAACC,GAAG,CAAE,YAAWF,GAAI,EAAC,CAAC;IAC9BC,OAAO,CAACC,GAAG,CAAC,0BAA0B,EAAEJ,KAAK,CAACL,MAAM,GAAGM,aAAa,CAAC;IACrE,IAAI;MACA,IAAII,OAAO,GAAG,MAAM5B,KAAK,CAAC6B,GAAG,CAACJ,GAAG,CAAC;IACtC,CAAC,CAAC,OAAOK,CAAC,EAAE;MACR,MAAMC,cAAc,CAAC,CAAC;MACtB;IACJ;IAGA,IAAI,CAACH,OAAO,CAACI,IAAI,EAAE;MACf,MAAMD,cAAc,CAAC,CAAC;MACtB;IACJ;IAEA,MAAME,CAAC,GAAGhC,OAAO,CAACiC,IAAI,CAACN,OAAO,CAACI,IAAI,CAAC,CAAC,CAAC;IACtC,MAAMG,KAAK,GAAGF,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IACtB,MAAMG,KAAK,GAAGH,CAAC,CAAC,IAAI,CAAC,CAACI,IAAI,CAAC,CAAC,CAAC,CAAC;;IAE9B;IACA,MAAMC,SAAS,GAAG,MAAMC,aAAa,CAACd,GAAG,CAAC;IAC1C,IAAI,CAACa,SAAS,EAAE;MACZ,MAAMP,cAAc,CAAC,CAAC;MACtB;IACJ;IAEAI,KAAK,CAACK,IAAI,CAAC,CAACC,KAAK,EAAEC,IAAI,KAAK;MACxB,MAAMC,IAAI,GAAGV,CAAC,CAACS,IAAI,CAAC,CAACE,IAAI,CAAC,MAAM,CAAC;MAEjC,IAAI,CAACD,IAAI,EAAE;;MAEX;MACA,IAAIA,IAAI,CAACE,UAAU,CAAC,SAAS,CAAC,IAAIF,IAAI,CAACE,UAAU,CAAC,UAAU,CAAC,EAAE;QAC3D;QACAC,OAAO,GAAG;UACNC,EAAE,EAAEjC,QAAQ,CAACW,GAAG,CAAC;UACjBW,KAAK,EAAEA,KAAK;UACZM,IAAI,EAAEjB,GAAG;UACTuB,KAAK,EAAE;QACX,CAAC;QACDC,sBAAsB,CAACH,OAAO,CAAC;QAC/B;MACJ;;MAEA;MACA,MAAMI,SAAS,GAAG,IAAIC,GAAG,CAAC1B,GAAG,CAAC,CAAC2B,MAAM;MACrC,MAAMC,MAAM,GAAGH,SAAS,GAAGP,IAAI;MAC/B;MACAG,OAAO,GAAG;QACNC,EAAE,EAAEjC,QAAQ,CAACuC,MAAM,CAAC;QACpBjB,KAAK,EAAEA,KAAK;QACZM,IAAI,EAAEW,MAAM;QACZL,KAAK,EAAE;MACX,CAAC;MACDC,sBAAsB,CAACH,OAAO,CAAC;IACnC,CAAC,CAAC;IAEF,IAAIvB,KAAK,CAACC,aAAa,CAAC,EAAE;MACtB,MAAMO,cAAc,CAAC,CAAC;IAC1B,CAAC,MAAM;MACHL,OAAO,CAACC,GAAG,CAAC,mBAAmB,CAAC;IACpC;EACJ,CAAC;;EAED;AACJ;AACA;AACA;AACA;EACI,eAAesB,sBAAsBA,CAACjB,IAAI,EAAE;IACxC,MAAMsB,KAAK,CAAE,gCAA+BtB,IAAI,CAACe,EAAG,EAAC,EAAE;MACnDQ,MAAM,EAAE;IACZ,CAAC,EAAE,UAAUC,GAAG,EAAEC,GAAG,EAAE;MACnB,IAAID,GAAG,EAAE,MAAMA,GAAG;MAClB,IAAIC,GAAG,EAAE;QACL,IAAIA,GAAG,IAAI,IAAI,EAAE;UACblC,KAAK,CAACmC,IAAI,CAAC1B,IAAI,CAAC;QACpB,CAAC,MAAM;UACH;UACAyB,GAAG,CAACT,KAAK,IAAI,CAAC;UACdW,WAAW,CAACF,GAAG,CAAC;QACpB;MACJ;IACJ,CACA,CAAC;EAGL;;EAIA;AACJ;AACA;EACI,MAAM1B,cAAc,GAAG,MAAAA,CAAA,KAAY;IAC/B,MAAM6B,OAAO,CAAC,CAAC;IACf,IAAIpD,UAAU,EAAE;MAChBE,KAAK,CAACa,KAAK,CAACC,aAAa,CAAC,CAAC;MAC3BqC,OAAO,CAACtC,KAAK,CAACC,aAAa,CAAC,CAAC;MAC7BA,aAAa,IAAI,CAAC;IAClB,CAAC,MAAM;MACH;IACJ;EACJ,CAAC;;EAED;AACJ;AACA;EACI,MAAMoC,OAAO,GAAGA,CAAA,KAAM;IAClB,OAAO,IAAIE,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;MACpCC,UAAU,CAAC,MAAM;QACbF,OAAO,CAAC,CAAC;MACb,CAAC,EAAE,GAAG,CAAC;IACX,CAAC,CAAC;EACN,CAAC;;EAED;AACJ;AACA;EACI,eAAeF,OAAOA,CAAC7B,IAAI,EAAE;IACzB,MAAMsB,KAAK,CAAE,kCAAiC,EAAE;MAC5CC,MAAM,EAAE,MAAM;MACdW,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACpC,IAAI;IAC7B,CAAC,EAAE,UAAUwB,GAAG,EAAEC,GAAG,EAAE;MACnB,IAAID,GAAG,EAAE,MAAMA,GAAG;MAClBa,QAAQ,CAACC,IAAI,CAACb,GAAG,CAAC;IACtB,CACA,CAAC;EACL;;EAEA;AACJ;AACA;AACA;AACA;EACI,eAAeE,WAAWA,CAAC3B,IAAI,EAAE;IAC7B,MAAMsB,KAAK,CAAE,gCAA+BtB,IAAI,CAACe,EAAG,EAAC,EAAE;MACnDQ,MAAM,EAAE,MAAM;MACdW,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACpC,IAAI;IAC7B,CAAC,EAAE,UAAUwB,GAAG,EAAEC,GAAG,EAAE;MACnB,IAAID,GAAG,EAAE,MAAMA,GAAG;MAClBa,QAAQ,CAACC,IAAI,CAACb,GAAG,CAAC;IACtB,CACA,CAAC;EACL;;EAEA;AACJ;AACA;AACA;AACA;EACI,MAAMlB,aAAa,GAAG,MAAOgC,UAAU,IAAK;IACxC,IAAIjC,SAAS,GAAG,IAAI;IACpB,KAAK,MAAMkC,GAAG,IAAIlD,cAAc,EAAE;MAC9B,IAAIiD,UAAU,CAACE,QAAQ,CAACD,GAAG,CAAC,EAAE;QAC1BlC,SAAS,GAAG,KAAK;QACjB;MACJ;IACJ;IACA,OAAOA,SAAS;EACpB,CAAC;;EAED;AACJ;AACA;AACA;EACI,MAAMoC,cAAc,GAAG,MAAOC,OAAO,IAAK;IACtC,MAAMC,QAAQ,GAAGD,OAAO,GAAG,aAAa;IACxC,MAAME,QAAQ,GAAG,MAAMvB,KAAK,CAACsB,QAAQ,CAAC;IACtC,MAAME,YAAY,GAAG,MAAMD,QAAQ,CAACxC,IAAI,CAAC,CAAC;IAC1C,MAAM0C,aAAa,GAAGD,YAAY,CAACE,KAAK,CAAC,IAAI,CAAC;IAC9CD,aAAa,CAACE,OAAO,CAACC,IAAI,IAAI;MAC1B,IAAIA,IAAI,CAACT,QAAQ,CAAC,WAAW,CAAC,EAAE;QAC5B,MAAMU,aAAa,GAAGD,IAAI,CAACF,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAGxC,IAAI,CAACG,aAAa,EAAE;UAChBzD,OAAO,CAACC,GAAG,CAAC,2BAA2B,CAAC;QAC5C,CAAC,MAAM;UACH,IAAI,CAACwD,aAAa,CAACV,QAAQ,CAAC,QAAQ,CAAC,EAAE;YACnC;YACA;UACJ;UACA/C,OAAO,CAACC,GAAG,CAAC,qBAAqB,EAAEwD,aAAa,CAAC;UACjD;UACA7D,cAAc,CAAC6D,aAAa,CAAC,GAAG;YAC5BC,WAAW,EAAED;UACjB,CAAC;UACD,MAAMb,IAAI,GAAGH,IAAI,CAACC,SAAS,CAAC9C,cAAc,CAAC;UAC3CpB,EAAE,CAACmF,aAAa,CAAC,mBAAmB,EAAEf,IAAI,CAAC;QAC/C;MACJ;IACJ,CAAC,CAAC;EACN,CAAC;EACDI,cAAc,CAACpE,OAAO,CAAC;EACvBiB,KAAK,CAACmC,IAAI,CAACpD,OAAO,CAAC;AACvB;AAACC,EAAA,CA1OuBF,WAAW;EAAA,QACZD,WAAW,EACbD,WAAW;AAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}