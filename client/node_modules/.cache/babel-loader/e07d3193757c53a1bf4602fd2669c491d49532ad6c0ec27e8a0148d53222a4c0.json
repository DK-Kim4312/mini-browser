{"ast":null,"code":"var _jsxFileName = \"/Users/kimdo/Documents/Projects/mini-browser/client/src/components/crawl/Crawl.jsx\",\n  _s = $RefreshSig$();\nimport React from 'react';\nimport { useSelector } from 'react-redux';\nimport { CrawlWrapper } from './Crawl.style';\nimport { useDispatch } from 'react-redux';\nimport { RESTING } from '../../reducer/crawl';\nimport { Spinner } from './Spinner';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst cheerio = require('cheerio');\nconst axios = require('axios');\nconst fs = require('fs');\nexport default function Crawl(initUrl = 'https://en.wikipedia.org') {\n  _s();\n  const isCrawling = useSelector(state => state.crawl.isCrawling);\n  const dispatch = useDispatch();\n  const stopCrawl = () => {\n    dispatch({\n      type: RESTING\n    });\n  };\n  if (!isCrawling) {\n    return null;\n  }\n  ;\n  dbName = \"title-link-score\";\n  collectionName = \"wikipedia\";\n  function hashCode(s) {\n    for (var i = 0, h = 0; i < s.length; i++) h = Math.imul(31, h) + s.charCodeAt(i) | 0;\n    return h;\n  }\n\n  // What we need to do for scraping\n  // 1. Visit the website\n  // 2. Get the HTML from the website (cheerio)\n  // 3. Find the element we want to scrape -> stort in DB ( we will use fs )\n  // 4. Find the next page url -> rank pages by reference\n  // 5. Go to the next page and repeat\n\n  let disallowedList = {\n    [\"/wiki/MediaWiki:Spam-blacklist\"]: {\n      'linkSegment': '/wiki/MediaWiki:Spam-blacklist'\n    }\n  };\n  let queue = [];\n  let progressIndex = 0;\n\n  /**\n   * crawl function\n   * @param {*} url \n   * @returns \n   */\n  const crawl = async url => {\n    console.log(`Visiting ${url}`);\n    console.log('Number of URL in queue: ', queue.length - progressIndex);\n    try {\n      var htmlDoc = await axios.get(url);\n    } catch (e) {\n      await startNextQueue();\n      return;\n    }\n    if (!htmlDoc.data) {\n      await startNextQueue();\n      return;\n    }\n    const $ = cheerio.load(htmlDoc.data); // conventional $ notation\n    const links = $('a'); // get all links from the page\n    const title = $(\"h1\").text(); // get title of the page\n\n    // check if the url is allowed to be crawled\n    const isAllowed = await checkRobotTXT(url);\n    if (!isAllowed) {\n      await startNextQueue();\n      return;\n    }\n    links.each((index, link) => {\n      const href = $(link).attr('href');\n      if (!href) return;\n\n      // if href starts with http or https\n      if (href.startsWith('http://') || href.startsWith('https://')) {\n        //create new data\n        newData = {\n          id: hashCode(url),\n          title: title,\n          link: url,\n          score: 1\n        };\n        checkVisitedBeforePush(newData);\n        return;\n      }\n\n      // if not, it is a relative link\n      const originUrl = new URL(url).origin;\n      const newUrl = originUrl + href;\n      //create new data\n      newData = {\n        id: hashCode(newUrl),\n        title: title,\n        link: newUrl,\n        score: 1\n      };\n      checkVisitedBeforePush(newData);\n    });\n    if (queue[progressIndex]) {\n      await startNextQueue();\n    } else {\n      console.log('Finished crawling');\n    }\n  };\n\n  /**\n   * Check if the url is visited before check in mongodb\n   * @param {object} data\n   * @returns {boolean}\n   */\n  async function checkVisitedBeforePush(data) {\n    await fetch(`http://localhost:3000/record/${data.id}`, {\n      method: \"GET\"\n    }, function (err, res) {\n      if (err) throw err;\n      if (res) {\n        if (res == null) {\n          queue.push(data);\n        } else {\n          // update score\n          res.score += 1;\n          updateScore(res);\n        }\n      }\n    });\n  }\n\n  /**\n   * Start crawling the next url in the queue\n   */\n  const startNextQueue = async () => {\n    await timeout();\n    crawl(queue[progressIndex]);\n    storeDb(queue[progressIndex]);\n    progressIndex += 1;\n  };\n\n  /**\n   * Timeout function\n   */\n  const timeout = () => {\n    return new Promise((resolve, reject) => {\n      setTimeout(() => {\n        resolve();\n      }, 300);\n    });\n  };\n\n  /**\n   * Store new data in mongodb\n   */\n  async function storeDb(data) {\n    await fetch(`http://localhost:3000/record/add`, {\n      method: \"POST\",\n      body: JSON.stringify(data)\n    }, function (err, res) {\n      if (err) throw err;\n      response.json(res);\n    });\n  }\n\n  /**\n   * Update score in mongodb\n   * @param {object} data\n   * @returns {boolean}\n   */\n  async function updateScore(data) {\n    await fetch(`http://localhost:3000/update/${data.id}`, {\n      method: \"POST\",\n      body: JSON.stringify(data)\n    }, function (err, res) {\n      if (err) throw err;\n      response.json(res);\n    });\n  }\n\n  /**\n   * Check if the url is allowed to be crawled based on disallowedList\n   * @param {string} currentUrl\n   * @returns {boolean}\n   */\n  const checkRobotTXT = async currentUrl => {\n    let isAllowed = true;\n    for (const key in disallowedList) {\n      if (currentUrl.includes(key)) {\n        isAllowed = false;\n        break;\n      }\n    }\n    return isAllowed;\n  };\n\n  /**\n   * Store disallowedList in a file\n   * @param {string} mainUrl\n   */\n  const storeRobotsTxt = async mainUrl => {\n    const robotUrl = mainUrl + '/robots.txt';\n    const robotTxt = await fetch(robotUrl);\n    const robotTxtText = await robotTxt.text();\n    const robotTxtLines = robotTxtText.split('\\n');\n    robotTxtLines.forEach(line => {\n      if (line.includes('Disallow:')) {\n        const disallowedUrl = line.split(' ')[1];\n        if (!disallowedUrl) {\n          console.log('Error Disallowed url text');\n        } else {\n          if (!disallowedUrl.includes('/wiki/')) {\n            //continue to next iteration\n            return;\n          }\n          console.log('Disallowed url text', disallowedUrl);\n          // store disallowedList in a file\n          disallowedList[disallowedUrl] = {\n            linkSegment: disallowedUrl\n          };\n          const json = JSON.stringify(disallowedList);\n          fs.writeFileSync('./disallowed.json', json);\n        }\n      }\n    });\n  };\n  storeRobotsTxt(initUrl);\n  crawl(initUrl);\n  return /*#__PURE__*/_jsxDEV(CrawlWrapper, {\n    children: /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"container\",\n      children: [/*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"header\",\n        children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n          children: \"Crawling\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 256,\n          columnNumber: 21\n        }, this), /*#__PURE__*/_jsxDEV(\"button\", {\n          className: \"Stop Crawling\",\n          onClick: stopCrawl,\n          children: \"Stop\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 257,\n          columnNumber: 21\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 255,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Spinner, {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 260,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 254,\n      columnNumber: 13\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 253,\n    columnNumber: 9\n  }, this);\n}\n_s(Crawl, \"UMFhAXZcMHuEO2P/OYVCp3ZKXnw=\", false, function () {\n  return [useSelector, useDispatch];\n});\n_c = Crawl;\nvar _c;\n$RefreshReg$(_c, \"Crawl\");","map":{"version":3,"names":["React","useSelector","CrawlWrapper","useDispatch","RESTING","Spinner","jsxDEV","_jsxDEV","cheerio","require","axios","fs","Crawl","initUrl","_s","isCrawling","state","crawl","dispatch","stopCrawl","type","dbName","collectionName","hashCode","s","i","h","length","Math","imul","charCodeAt","disallowedList","queue","progressIndex","url","console","log","htmlDoc","get","e","startNextQueue","data","$","load","links","title","text","isAllowed","checkRobotTXT","each","index","link","href","attr","startsWith","newData","id","score","checkVisitedBeforePush","originUrl","URL","origin","newUrl","fetch","method","err","res","push","updateScore","timeout","storeDb","Promise","resolve","reject","setTimeout","body","JSON","stringify","response","json","currentUrl","key","includes","storeRobotsTxt","mainUrl","robotUrl","robotTxt","robotTxtText","robotTxtLines","split","forEach","line","disallowedUrl","linkSegment","writeFileSync","children","className","fileName","_jsxFileName","lineNumber","columnNumber","onClick","_c","$RefreshReg$"],"sources":["/Users/kimdo/Documents/Projects/mini-browser/client/src/components/crawl/Crawl.jsx"],"sourcesContent":["import React from 'react';\nimport { useSelector } from 'react-redux';\nimport { CrawlWrapper } from './Crawl.style';\nimport { useDispatch } from 'react-redux';\nimport { RESTING } from '../../reducer/crawl';\nimport { Spinner } from './Spinner';\nconst cheerio = require('cheerio');\nconst axios = require('axios');\nconst fs = require('fs');\n\n\nexport default function Crawl(initUrl = 'https://en.wikipedia.org') {\n    const isCrawling = useSelector((state) => state.crawl.isCrawling);\n    const dispatch = useDispatch();\n\n    const stopCrawl = () => {\n        dispatch({ type: RESTING });\n    }\n\n    if (!isCrawling) { return null };\n\n    dbName = \"title-link-score\";\n    collectionName = \"wikipedia\";\n\n    function hashCode(s) {\n        for (var i = 0, h = 0; i < s.length; i++)\n            h = Math.imul(31, h) + s.charCodeAt(i) | 0;\n        return h;\n    }\n\n    // What we need to do for scraping\n    // 1. Visit the website\n    // 2. Get the HTML from the website (cheerio)\n    // 3. Find the element we want to scrape -> stort in DB ( we will use fs )\n    // 4. Find the next page url -> rank pages by reference\n    // 5. Go to the next page and repeat\n\n    let disallowedList = {\n        [\"/wiki/MediaWiki:Spam-blacklist\"]: {\n            'linkSegment': '/wiki/MediaWiki:Spam-blacklist',\n        }\n    };\n\n    let queue = [];\n    let progressIndex = 0;\n\n    /**\n     * crawl function\n     * @param {*} url \n     * @returns \n     */\n    const crawl = async (url) => {\n        console.log(`Visiting ${url}`);\n        console.log('Number of URL in queue: ', queue.length - progressIndex);\n        try {\n            var htmlDoc = await axios.get(url);\n        } catch (e) {\n            await startNextQueue();\n            return;\n        }\n\n\n        if (!htmlDoc.data) {\n            await startNextQueue();\n            return;\n        }\n\n        const $ = cheerio.load(htmlDoc.data); // conventional $ notation\n        const links = $('a'); // get all links from the page\n        const title = $(\"h1\").text(); // get title of the page\n\n        // check if the url is allowed to be crawled\n        const isAllowed = await checkRobotTXT(url);\n        if (!isAllowed) {\n            await startNextQueue();\n            return;\n        }\n\n        links.each((index, link) => {\n            const href = $(link).attr('href');\n\n            if (!href) return;\n\n            // if href starts with http or https\n            if (href.startsWith('http://') || href.startsWith('https://')) {\n                //create new data\n                newData = {\n                    id: hashCode(url),\n                    title: title,\n                    link: url,\n                    score: 1,\n                }\n                checkVisitedBeforePush(newData);\n                return;\n            }\n\n            // if not, it is a relative link\n            const originUrl = new URL(url).origin;\n            const newUrl = originUrl + href;\n            //create new data\n            newData = {\n                id: hashCode(newUrl),\n                title: title,\n                link: newUrl,\n                score: 1,\n            }\n            checkVisitedBeforePush(newData);\n        });\n\n        if (queue[progressIndex]) {\n            await startNextQueue();\n        } else {\n            console.log('Finished crawling');\n        }\n    };\n\n    /**\n     * Check if the url is visited before check in mongodb\n     * @param {object} data\n     * @returns {boolean}\n     */\n    async function checkVisitedBeforePush(data) {\n        await fetch(`http://localhost:3000/record/${data.id}`, {\n            method: \"GET\",\n        }, function (err, res) {\n            if (err) throw err;\n            if (res) {\n                if (res == null) {\n                    queue.push(data);\n                } else {\n                    // update score\n                    res.score += 1;\n                    updateScore(res);\n                }\n            }\n        }\n        );\n\n\n    }\n\n    /**\n     * Start crawling the next url in the queue\n     */\n    const startNextQueue = async () => {\n        await timeout();\n        crawl(queue[progressIndex]);\n        storeDb(queue[progressIndex]);\n        progressIndex += 1;\n\n    }\n\n    /**\n     * Timeout function\n     */\n    const timeout = () => {\n        return new Promise((resolve, reject) => {\n            setTimeout(() => {\n                resolve();\n            }, 300);\n        })\n    }\n\n    /**\n     * Store new data in mongodb\n     */\n    async function storeDb(data) {\n        await fetch(`http://localhost:3000/record/add`, {\n            method: \"POST\",\n            body: JSON.stringify(data),\n        }, function (err, res) {\n            if (err) throw err;\n            response.json(res);\n        }\n        );\n    }\n\n    /**\n     * Update score in mongodb\n     * @param {object} data\n     * @returns {boolean}\n     */\n    async function updateScore(data) {\n        await fetch(`http://localhost:3000/update/${data.id}`, {\n            method: \"POST\",\n            body: JSON.stringify(data),\n        }, function (err, res) {\n            if (err) throw err;\n            response.json(res);\n        }\n        );\n    }\n\n\n\n\n\n\n    /**\n     * Check if the url is allowed to be crawled based on disallowedList\n     * @param {string} currentUrl\n     * @returns {boolean}\n     */\n    const checkRobotTXT = async (currentUrl) => {\n        let isAllowed = true;\n        for (const key in disallowedList) {\n            if (currentUrl.includes(key)) {\n                isAllowed = false;\n                break;\n            }\n        }\n        return isAllowed;\n    }\n\n    /**\n     * Store disallowedList in a file\n     * @param {string} mainUrl\n     */\n    const storeRobotsTxt = async (mainUrl) => {\n        const robotUrl = mainUrl + '/robots.txt';\n        const robotTxt = await fetch(robotUrl);\n        const robotTxtText = await robotTxt.text();\n        const robotTxtLines = robotTxtText.split('\\n');\n        robotTxtLines.forEach(line => {\n            if (line.includes('Disallow:')) {\n                const disallowedUrl = line.split(' ')[1];\n\n\n                if (!disallowedUrl) {\n                    console.log('Error Disallowed url text');\n                } else {\n                    if (!disallowedUrl.includes('/wiki/')) {\n                        //continue to next iteration\n                        return;\n                    }\n                    console.log('Disallowed url text', disallowedUrl);\n                    // store disallowedList in a file\n                    disallowedList[disallowedUrl] = {\n                        linkSegment: disallowedUrl,\n                    }\n                    const json = JSON.stringify(disallowedList);\n                    fs.writeFileSync('./disallowed.json', json);\n                }\n            }\n        });\n    }\n\n\n    storeRobotsTxt(initUrl);\n    crawl(initUrl);\n\n    return (\n        <CrawlWrapper>\n            <div className=\"container\" >\n                <div className=\"header\" >\n                    <h1>Crawling</h1>\n                    <button className=\"Stop Crawling\"\n                        onClick={stopCrawl} >Stop</button>\n                </div>\n                <Spinner></Spinner>\n            </div>\n\n        </CrawlWrapper>\n    );\n\n}\n\n"],"mappings":";;AAAA,OAAOA,KAAK,MAAM,OAAO;AACzB,SAASC,WAAW,QAAQ,aAAa;AACzC,SAASC,YAAY,QAAQ,eAAe;AAC5C,SAASC,WAAW,QAAQ,aAAa;AACzC,SAASC,OAAO,QAAQ,qBAAqB;AAC7C,SAASC,OAAO,QAAQ,WAAW;AAAC,SAAAC,MAAA,IAAAC,OAAA;AACpC,MAAMC,OAAO,GAAGC,OAAO,CAAC,SAAS,CAAC;AAClC,MAAMC,KAAK,GAAGD,OAAO,CAAC,OAAO,CAAC;AAC9B,MAAME,EAAE,GAAGF,OAAO,CAAC,IAAI,CAAC;AAGxB,eAAe,SAASG,KAAKA,CAACC,OAAO,GAAG,0BAA0B,EAAE;EAAAC,EAAA;EAChE,MAAMC,UAAU,GAAGd,WAAW,CAAEe,KAAK,IAAKA,KAAK,CAACC,KAAK,CAACF,UAAU,CAAC;EACjE,MAAMG,QAAQ,GAAGf,WAAW,CAAC,CAAC;EAE9B,MAAMgB,SAAS,GAAGA,CAAA,KAAM;IACpBD,QAAQ,CAAC;MAAEE,IAAI,EAAEhB;IAAQ,CAAC,CAAC;EAC/B,CAAC;EAED,IAAI,CAACW,UAAU,EAAE;IAAE,OAAO,IAAI;EAAC;EAAC;EAEhCM,MAAM,GAAG,kBAAkB;EAC3BC,cAAc,GAAG,WAAW;EAE5B,SAASC,QAAQA,CAACC,CAAC,EAAE;IACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEC,CAAC,GAAG,CAAC,EAAED,CAAC,GAAGD,CAAC,CAACG,MAAM,EAAEF,CAAC,EAAE,EACpCC,CAAC,GAAGE,IAAI,CAACC,IAAI,CAAC,EAAE,EAAEH,CAAC,CAAC,GAAGF,CAAC,CAACM,UAAU,CAACL,CAAC,CAAC,GAAG,CAAC;IAC9C,OAAOC,CAAC;EACZ;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA,IAAIK,cAAc,GAAG;IACjB,CAAC,gCAAgC,GAAG;MAChC,aAAa,EAAE;IACnB;EACJ,CAAC;EAED,IAAIC,KAAK,GAAG,EAAE;EACd,IAAIC,aAAa,GAAG,CAAC;;EAErB;AACJ;AACA;AACA;AACA;EACI,MAAMhB,KAAK,GAAG,MAAOiB,GAAG,IAAK;IACzBC,OAAO,CAACC,GAAG,CAAE,YAAWF,GAAI,EAAC,CAAC;IAC9BC,OAAO,CAACC,GAAG,CAAC,0BAA0B,EAAEJ,KAAK,CAACL,MAAM,GAAGM,aAAa,CAAC;IACrE,IAAI;MACA,IAAII,OAAO,GAAG,MAAM3B,KAAK,CAAC4B,GAAG,CAACJ,GAAG,CAAC;IACtC,CAAC,CAAC,OAAOK,CAAC,EAAE;MACR,MAAMC,cAAc,CAAC,CAAC;MACtB;IACJ;IAGA,IAAI,CAACH,OAAO,CAACI,IAAI,EAAE;MACf,MAAMD,cAAc,CAAC,CAAC;MACtB;IACJ;IAEA,MAAME,CAAC,GAAGlC,OAAO,CAACmC,IAAI,CAACN,OAAO,CAACI,IAAI,CAAC,CAAC,CAAC;IACtC,MAAMG,KAAK,GAAGF,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IACtB,MAAMG,KAAK,GAAGH,CAAC,CAAC,IAAI,CAAC,CAACI,IAAI,CAAC,CAAC,CAAC,CAAC;;IAE9B;IACA,MAAMC,SAAS,GAAG,MAAMC,aAAa,CAACd,GAAG,CAAC;IAC1C,IAAI,CAACa,SAAS,EAAE;MACZ,MAAMP,cAAc,CAAC,CAAC;MACtB;IACJ;IAEAI,KAAK,CAACK,IAAI,CAAC,CAACC,KAAK,EAAEC,IAAI,KAAK;MACxB,MAAMC,IAAI,GAAGV,CAAC,CAACS,IAAI,CAAC,CAACE,IAAI,CAAC,MAAM,CAAC;MAEjC,IAAI,CAACD,IAAI,EAAE;;MAEX;MACA,IAAIA,IAAI,CAACE,UAAU,CAAC,SAAS,CAAC,IAAIF,IAAI,CAACE,UAAU,CAAC,UAAU,CAAC,EAAE;QAC3D;QACAC,OAAO,GAAG;UACNC,EAAE,EAAEjC,QAAQ,CAACW,GAAG,CAAC;UACjBW,KAAK,EAAEA,KAAK;UACZM,IAAI,EAAEjB,GAAG;UACTuB,KAAK,EAAE;QACX,CAAC;QACDC,sBAAsB,CAACH,OAAO,CAAC;QAC/B;MACJ;;MAEA;MACA,MAAMI,SAAS,GAAG,IAAIC,GAAG,CAAC1B,GAAG,CAAC,CAAC2B,MAAM;MACrC,MAAMC,MAAM,GAAGH,SAAS,GAAGP,IAAI;MAC/B;MACAG,OAAO,GAAG;QACNC,EAAE,EAAEjC,QAAQ,CAACuC,MAAM,CAAC;QACpBjB,KAAK,EAAEA,KAAK;QACZM,IAAI,EAAEW,MAAM;QACZL,KAAK,EAAE;MACX,CAAC;MACDC,sBAAsB,CAACH,OAAO,CAAC;IACnC,CAAC,CAAC;IAEF,IAAIvB,KAAK,CAACC,aAAa,CAAC,EAAE;MACtB,MAAMO,cAAc,CAAC,CAAC;IAC1B,CAAC,MAAM;MACHL,OAAO,CAACC,GAAG,CAAC,mBAAmB,CAAC;IACpC;EACJ,CAAC;;EAED;AACJ;AACA;AACA;AACA;EACI,eAAesB,sBAAsBA,CAACjB,IAAI,EAAE;IACxC,MAAMsB,KAAK,CAAE,gCAA+BtB,IAAI,CAACe,EAAG,EAAC,EAAE;MACnDQ,MAAM,EAAE;IACZ,CAAC,EAAE,UAAUC,GAAG,EAAEC,GAAG,EAAE;MACnB,IAAID,GAAG,EAAE,MAAMA,GAAG;MAClB,IAAIC,GAAG,EAAE;QACL,IAAIA,GAAG,IAAI,IAAI,EAAE;UACblC,KAAK,CAACmC,IAAI,CAAC1B,IAAI,CAAC;QACpB,CAAC,MAAM;UACH;UACAyB,GAAG,CAACT,KAAK,IAAI,CAAC;UACdW,WAAW,CAACF,GAAG,CAAC;QACpB;MACJ;IACJ,CACA,CAAC;EAGL;;EAEA;AACJ;AACA;EACI,MAAM1B,cAAc,GAAG,MAAAA,CAAA,KAAY;IAC/B,MAAM6B,OAAO,CAAC,CAAC;IACfpD,KAAK,CAACe,KAAK,CAACC,aAAa,CAAC,CAAC;IAC3BqC,OAAO,CAACtC,KAAK,CAACC,aAAa,CAAC,CAAC;IAC7BA,aAAa,IAAI,CAAC;EAEtB,CAAC;;EAED;AACJ;AACA;EACI,MAAMoC,OAAO,GAAGA,CAAA,KAAM;IAClB,OAAO,IAAIE,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;MACpCC,UAAU,CAAC,MAAM;QACbF,OAAO,CAAC,CAAC;MACb,CAAC,EAAE,GAAG,CAAC;IACX,CAAC,CAAC;EACN,CAAC;;EAED;AACJ;AACA;EACI,eAAeF,OAAOA,CAAC7B,IAAI,EAAE;IACzB,MAAMsB,KAAK,CAAE,kCAAiC,EAAE;MAC5CC,MAAM,EAAE,MAAM;MACdW,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACpC,IAAI;IAC7B,CAAC,EAAE,UAAUwB,GAAG,EAAEC,GAAG,EAAE;MACnB,IAAID,GAAG,EAAE,MAAMA,GAAG;MAClBa,QAAQ,CAACC,IAAI,CAACb,GAAG,CAAC;IACtB,CACA,CAAC;EACL;;EAEA;AACJ;AACA;AACA;AACA;EACI,eAAeE,WAAWA,CAAC3B,IAAI,EAAE;IAC7B,MAAMsB,KAAK,CAAE,gCAA+BtB,IAAI,CAACe,EAAG,EAAC,EAAE;MACnDQ,MAAM,EAAE,MAAM;MACdW,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACpC,IAAI;IAC7B,CAAC,EAAE,UAAUwB,GAAG,EAAEC,GAAG,EAAE;MACnB,IAAID,GAAG,EAAE,MAAMA,GAAG;MAClBa,QAAQ,CAACC,IAAI,CAACb,GAAG,CAAC;IACtB,CACA,CAAC;EACL;;EAOA;AACJ;AACA;AACA;AACA;EACI,MAAMlB,aAAa,GAAG,MAAOgC,UAAU,IAAK;IACxC,IAAIjC,SAAS,GAAG,IAAI;IACpB,KAAK,MAAMkC,GAAG,IAAIlD,cAAc,EAAE;MAC9B,IAAIiD,UAAU,CAACE,QAAQ,CAACD,GAAG,CAAC,EAAE;QAC1BlC,SAAS,GAAG,KAAK;QACjB;MACJ;IACJ;IACA,OAAOA,SAAS;EACpB,CAAC;;EAED;AACJ;AACA;AACA;EACI,MAAMoC,cAAc,GAAG,MAAOC,OAAO,IAAK;IACtC,MAAMC,QAAQ,GAAGD,OAAO,GAAG,aAAa;IACxC,MAAME,QAAQ,GAAG,MAAMvB,KAAK,CAACsB,QAAQ,CAAC;IACtC,MAAME,YAAY,GAAG,MAAMD,QAAQ,CAACxC,IAAI,CAAC,CAAC;IAC1C,MAAM0C,aAAa,GAAGD,YAAY,CAACE,KAAK,CAAC,IAAI,CAAC;IAC9CD,aAAa,CAACE,OAAO,CAACC,IAAI,IAAI;MAC1B,IAAIA,IAAI,CAACT,QAAQ,CAAC,WAAW,CAAC,EAAE;QAC5B,MAAMU,aAAa,GAAGD,IAAI,CAACF,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAGxC,IAAI,CAACG,aAAa,EAAE;UAChBzD,OAAO,CAACC,GAAG,CAAC,2BAA2B,CAAC;QAC5C,CAAC,MAAM;UACH,IAAI,CAACwD,aAAa,CAACV,QAAQ,CAAC,QAAQ,CAAC,EAAE;YACnC;YACA;UACJ;UACA/C,OAAO,CAACC,GAAG,CAAC,qBAAqB,EAAEwD,aAAa,CAAC;UACjD;UACA7D,cAAc,CAAC6D,aAAa,CAAC,GAAG;YAC5BC,WAAW,EAAED;UACjB,CAAC;UACD,MAAMb,IAAI,GAAGH,IAAI,CAACC,SAAS,CAAC9C,cAAc,CAAC;UAC3CpB,EAAE,CAACmF,aAAa,CAAC,mBAAmB,EAAEf,IAAI,CAAC;QAC/C;MACJ;IACJ,CAAC,CAAC;EACN,CAAC;EAGDI,cAAc,CAACtE,OAAO,CAAC;EACvBI,KAAK,CAACJ,OAAO,CAAC;EAEd,oBACIN,OAAA,CAACL,YAAY;IAAA6F,QAAA,eACTxF,OAAA;MAAKyF,SAAS,EAAC,WAAW;MAAAD,QAAA,gBACtBxF,OAAA;QAAKyF,SAAS,EAAC,QAAQ;QAAAD,QAAA,gBACnBxF,OAAA;UAAAwF,QAAA,EAAI;QAAQ;UAAAE,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC,eACjB7F,OAAA;UAAQyF,SAAS,EAAC,eAAe;UAC7BK,OAAO,EAAElF,SAAU;UAAA4E,QAAA,EAAE;QAAI;UAAAE,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OACrC,CAAC,eACN7F,OAAA,CAACF,OAAO;QAAA4F,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAU,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAClB;EAAC;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OAEI,CAAC;AAGvB;AAACtF,EAAA,CA9PuBF,KAAK;EAAA,QACNX,WAAW,EACbE,WAAW;AAAA;AAAAmG,EAAA,GAFR1F,KAAK;AAAA,IAAA0F,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}